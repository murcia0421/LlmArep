# Lab Statement: Introduction to Creating RAGs (Retrieval-Augmented Generators) with OpenAI

## Project Description
This project implements a Retrieval-Augmented Generator (RAG) using OpenAI for embeddings and language models (LLMs), and Pinecone as the vector database. The goal is to build a system that combines contextual information retrieval with advanced language model responses.

The project follows LangChain tutorials to build an LLM chain and a RAG, and uses Pinecone to efficiently store and retrieve embeddings.

## Project Architecture
The project consists of the following main components:

### OpenAI Embeddings:
- The `text-embedding-3-large` model from OpenAI is used to generate document embeddings.
- These embeddings represent the content of the documents in a vector space.

### Pinecone:
- Pinecone is used as the vector database to store and retrieve embeddings efficiently.
- The embeddings generated by OpenAI are stored in a Pinecone index for later retrieval.

### LangChain:
- LangChain is used to integrate OpenAI's language model (LLM) with the information retrieval mechanism.
- The RAG combines document retrieval with contextual response generation.

### Workflow:
1. **Document Loading:** Documents are loaded from a source (e.g., a website) and converted into embeddings.
2. **Storage in Pinecone:** The embeddings are stored in a Pinecone index.
3. **Retrieval and Generation:** When a query is made, the most relevant documents are retrieved using Pinecone, and a contextual response is generated using OpenAI's language model.

## Prerequisites
Before running the code, ensure you have the following dependencies installed:

- Python 3.8 or higher.
- Python libraries: `langchain`, `openai`, `pinecone-client`, `python-dotenv`.

You can install the dependencies by running:

```bash
pip install langchain openai pinecone-client python-dotenv
```

## Project Setup

### API Keys:
Create a `.env` file in the root of the project and add your API keys:

```plaintext
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX_NAME=your_pinecone_index_name
```

Make sure the `.env` file is included in your `.gitignore` to avoid uploading your credentials to GitHub.

### Pinecone Setup:
Create a Pinecone index with the following specifications:

- **Dimension:** 1536 (compatible with the `text-embedding-3-large` model).
- **Metric:** cosine.

You can create the index from the Pinecone console or using the following code:

```python
import pinecone
import os

pinecone.init(api_key=os.getenv("PINECONE_API_KEY"), environment=os.getenv("PINECONE_ENVIRONMENT"))
index_name = os.getenv("PINECONE_INDEX_NAME")
if index_name not in pinecone.list_indexes():
    pinecone.create_index(name=index_name, dimension=1536, metric="cosine")
```

## Running the Code

### Loading Documents:
Documents are loaded from a source (e.g., a website) using `WebBaseLoader` from LangChain.

```python
from langchain.document_loaders import WebBaseLoader

loader = WebBaseLoader(["https://www.eltiempo.com/noticias"])
docs = loader.load()
```

### Generating Embeddings:
Documents are converted into embeddings using OpenAI.

```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

### Storing in Pinecone:
The embeddings are stored in a Pinecone index.

```python
from langchain.vectorstores import Pinecone

vector_store = Pinecone.from_documents(docs, embeddings, index_name=index_name)
```

### Retrieval and Generation:
When a query is made, the most relevant documents are retrieved, and a contextual response is generated.

```python
query = "What is dark energy?"
retrieved_docs = vector_store.similarity_search(query)
```

## Usage Example
Hereâ€™s an example of how to use the RAG to answer a query:

```python
from langchain.chat_models import ChatOpenAI

query = "What is dark energy?"
retrieved_docs = vector_store.similarity_search(query)

llm = ChatOpenAI(model="gpt-4")
response = llm.predict(f"Answer the following question based on these documents: {retrieved_docs}\n\nQuestion: {query}")
print(response)
```
Screenshot of Functionality

Below is a screenshot showcasing the functionality of the system running with Pinecone:

![image](https://github.com/user-attachments/assets/78f9376c-6cb8-4632-b9f8-23fdf300ec26)
![image](https://github.com/user-attachments/assets/0733ab1f-8a87-463e-90b0-6609116fd034)


## Assessment
This project will be evaluated based on:

- **Code Completeness:** The code should follow the LangChain tutorials and work correctly.
- **Documentation:** The `README.md` should be clear and detailed, with step-by-step instructions.
- **Repository Organization:** The repository should be well-organized and presented.

## Useful Links
- [LangChain LLM Chain Tutorial](https://python.langchain.com/en/latest/modules/chains/index.html)
- [LangChain RAG Tutorial](https://python.langchain.com/en/latest/use_cases/question_answering.html)
- [Pinecone Documentation](https://www.pinecone.io/docs/)

---

## Author

- Juan Daniel Murcia
- GitHub: murcia0421







   
